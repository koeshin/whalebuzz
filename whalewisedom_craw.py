import pandas as pd
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from io import StringIO
from concurrent.futures import ThreadPoolExecutor
import time

TARGETS = [
    {"name": "Berkshire Hathaway", "slug": "berkshire-hathaway-inc"},
    {"name": "Bridgewater", "slug": "bridgewater-associates-lp"},
    {"name": "Scion Asset", "slug": "scion-asset-management-llc"},
]

def scrape_single_filer(target, headless=True):
    """ë‹¨ì¼ ìš´ìš©ì‚¬ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        
        # ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ìœ¼ë¡œ ì†ë„ í–¥ìƒ
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        
        page = context.new_page()
        
        # ì´ë¯¸ì§€, í°íŠ¸, CSS ì°¨ë‹¨ (ì„ íƒì )
        page.route("**/*.{png,jpg,jpeg,gif,svg,css,woff,woff2}", lambda route: route.abort())
        
        url = f"https://whalewisdom.com/filer/{target['slug']}"
        print(f"[{target['name']}] í¬ë¡¤ë§ ì‹œì‘...")
        
        try:
            # í˜ì´ì§€ ë¡œë”© (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
            page.goto(url, wait_until='domcontentloaded', timeout=30000)
            
            # í…Œì´ë¸” ëŒ€ê¸° (ë™ì )
            page.wait_for_selector("#holdings_table", timeout=10000)
            
            # í•„ìš”ì‹œì—ë§Œ ìŠ¤í¬ë¡¤
            if page.locator(".lazy-load").count() > 0:
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(1000)
            
            # HTML íŒŒì‹±
            html = page.content()
            soup = BeautifulSoup(html, 'html.parser')
            
            # í…Œì´ë¸” ì¶”ì¶œ
            table = soup.select_one("#holdings_table")
            if not table:
                print(f"[{target['name']}] í…Œì´ë¸” ì—†ìŒ")
                return None
            
            dfs = pd.read_html(StringIO(str(table)))
            
            if dfs:
                df = dfs[0].dropna(axis=1, how='all')
                df.insert(0, "Manager", target['name'])
                top20 = df.head(20)
                
                filename = f"Whale_{target['slug']}.csv"
                top20.to_csv(filename, index=False, encoding='utf-8-sig')
                print(f"âœ… [{target['name']}] ì™„ë£Œ ({len(df)}ê°œ ì¤‘ 20ê°œ ì €ì¥)")
                return top20
            
        except Exception as e:
            print(f"âŒ [{target['name']}] ì—ëŸ¬: {e}")
            return None
        finally:
            browser.close()

def scrape_whalewisdom_fast(parallel=False, headless=True):
    """
    parallel=True: ë³‘ë ¬ ì²˜ë¦¬ (ë¹ ë¦„, but ì„œë²„ ë¶€í•˜ ì£¼ì˜)
    headless=True: GUI ì—†ì´ ì‹¤í–‰ (Cloudflare ì—†ì„ ë•Œë§Œ)
    """
    if parallel:
        print("âš¡ ë³‘ë ¬ ëª¨ë“œ (ìµœëŒ€ 3ê°œ ë™ì‹œ ì‹¤í–‰)")
        with ThreadPoolExecutor(max_workers=3) as executor:
            results = list(executor.map(
                lambda t: scrape_single_filer(t, headless), 
                TARGETS
            ))
        return [r for r in results if r is not None]
    else:
        print("ğŸŒ ìˆœì°¨ ëª¨ë“œ")
        results = []
        for target in TARGETS:
            result = scrape_single_filer(target, headless)
            if result is not None:
                results.append(result)
            time.sleep(1)  # ì„œë²„ ë¶€ë‹´ ì™„í™”
        return results

if __name__ == "__main__":
    # ì˜µì…˜ 1: ë³‘ë ¬ + headless (ê°€ì¥ ë¹ ë¦„, Cloudflare ì—†ì„ ë•Œ)
    # results = scrape_whalewisdom_fast(parallel=True, headless=True)
    
    # ì˜µì…˜ 2: ìˆœì°¨ + headless (ì•ˆì •ì )
    # results = scrape_whalewisdom_fast(parallel=False, headless=True)
    
    # ì˜µì…˜ 3: ìˆœì°¨ + GUI (Cloudflare ìˆì„ ë•Œ)
    results = scrape_whalewisdom_fast(parallel=False, headless=False)
    
    print(f"\nğŸ“Š ì´ {len(results)}ê°œ ìš´ìš©ì‚¬ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")