"""
Reddit ì£¼ì‹ í‹°ì»¤ í¬ë¡¤ëŸ¬ - PullPush.io API ì‚¬ìš©

PullPush.ioëŠ” Pushshiftì˜ í›„ì† ì„œë¹„ìŠ¤ë¡œ 2023ë…„ ë°ì´í„° í¬ë¡¤ë§ ê°€ëŠ¥
API í‚¤ ë¶ˆí•„ìš”, ë¬´ë£Œ ì‚¬ìš© ê°€ëŠ¥
"""

import requests
import pandas as pd
import re
from datetime import datetime, timedelta
import time
import json
from typing import List, Dict, Set
from collections import defaultdict

class RedditTickerCrawler:
    """
    PullPush.io APIë¥¼ ì‚¬ìš©í•œ Reddit í¬ë¡¤ëŸ¬
    - 2023ë…„ ë¶„ê¸°ë³„ ë°ì´í„° í¬ë¡¤ë§ ê°€ëŠ¥
    - API í‚¤ ë¶ˆí•„ìš”
    - Rate Limit: 15 req/min (soft), 30 req/min (hard), 1000 req/hr (ì¥ê¸°)
    """
    
    def __init__(self):
        self.base_url = "https://api.pullpush.io/reddit/search/submission"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'RedditTickerCrawler/1.0'
        })
        
        # Rate limiting ê´€ë¦¬
        self.request_count = 0
        self.request_times = []
        
        self.subreddits_config = {
            'wallstreetbets': {
                'style': 'ê´‘ê¸° & í•˜ì´í”„',
                'strategy': 'Activist / Growth',
                'characteristics': 'ë‹¨ê¸° ê¸‰ë“±(Pump), ìˆ ìŠ¤í€´ì¦ˆ, ë°ˆ(Meme) í™”ë ¥ ì¸¡ì •'
            },
            'stocks': {
                'style': 'ì¼ë°˜ íˆ¬ì í† ë¡ ',
                'strategy': 'All Round',
                'characteristics': 'ê· í˜• ì¡íŒ ì‹œê°, ì§„ì§€í•œ ë‰´ìŠ¤ ê³µìœ '
            },
            'investing': {
                'style': 'ì¥ê¸°/í€ë”ë©˜í„¸',
                'strategy': 'Value',
                'characteristics': 'ê°€ì¹˜ì£¼ ì‹¬ì¸µ ë¶„ì„'
            }
        }
    
    def rate_limit_wait(self):
        """Rate limit ê´€ë¦¬ (15 req/min soft limit)"""
        current_time = time.time()
        
        # 1ë¶„ ì´ë‚´ì˜ ìš”ì²­ë§Œ ìœ ì§€
        self.request_times = [t for t in self.request_times if current_time - t < 60]
        
        # 15ê°œ ì´ìƒì´ë©´ ëŒ€ê¸°
        if len(self.request_times) >= 14:
            wait_time = 60 - (current_time - self.request_times[0]) + 1
            if wait_time > 0:
                print(f"  Rate limit ëŒ€ê¸°: {wait_time:.1f}ì´ˆ")
                time.sleep(wait_time)
                self.request_times = []
        
        # ìš”ì²­ ê°„ ìµœì†Œ ê°„ê²© (ì•ˆì „í•˜ê²Œ 4ì´ˆ)
        time.sleep(4)
        self.request_times.append(time.time())
    
    def extract_tickers(self, text: str, target_tickers: Set[str]) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ íƒ€ê²Ÿ í‹°ì»¤ ì¶”ì¶œ"""
        if not text:
            return []
        text_upper = text.upper()
        found = []
        for ticker in target_tickers:
            # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ë§¤ì¹­
            if re.search(r'\b' + re.escape(ticker) + r'\b', text_upper):
                found.append(ticker)
        return found
    
    def get_quarter_timestamps(self, year: int, quarter: int):
        """ë¶„ê¸°ì˜ ì‹œì‘/ì¢…ë£Œ Unix timestamp ë°˜í™˜"""
        quarter_starts = {
            1: (1, 1), 2: (4, 1), 3: (7, 1), 4: (10, 1)
        }
        
        start_month, start_day = quarter_starts[quarter]
        start_date = datetime(year, start_month, start_day)
        
        if quarter == 4:
            end_date = datetime(year + 1, 1, 1) - timedelta(seconds=1)
        else:
            next_month, _ = quarter_starts[quarter + 1]
            end_date = datetime(year, next_month, 1) - timedelta(seconds=1)
        
        return int(start_date.timestamp()), int(end_date.timestamp())
    
    def crawl_quarter(self, subreddit_name: str, year: int, quarter: int,
                      target_tickers: Set[str], target_count: int = 1000) -> List[Dict]:
        """
        íŠ¹ì • ì„œë¸Œë ˆë”§ì˜ ë¶„ê¸°ë³„ ë°ì´í„° í¬ë¡¤ë§
        
        Args:
            subreddit_name: ì„œë¸Œë ˆë”§ ì´ë¦„
            year: ì—°ë„
            quarter: ë¶„ê¸° (1-4)
            target_tickers: ì°¾ì„ í‹°ì»¤ ì„¸íŠ¸
            target_count: ëª©í‘œ ë°ì´í„° ìˆ˜ (í‹°ì»¤ ë§¤ì¹­ëœ ê²ƒ)
            
        Returns:
            í¬ë¡¤ë§ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        print(f"\n{'='*60}")
        print(f"í¬ë¡¤ë§: r/{subreddit_name} - {year}ë…„ Q{quarter}")
        print(f"{'='*60}")
        
        start_ts, end_ts = self.get_quarter_timestamps(year, quarter)
        results = []
        
        params = {
            'subreddit': subreddit_name,
            'after': start_ts,
            'before': end_ts,
            'sort': 'desc',
            'sort_type': 'score',  # ì¸ê¸°ìˆœ (score ê¸°ì¤€)
            'size': 100  # í•œ ë²ˆì— 100ê°œì”© ê°€ì ¸ì˜¤ê¸°
        }
        
        matched_count = 0
        total_processed = 0
        before_timestamp = end_ts
        
        while matched_count < target_count:
            params['before'] = before_timestamp
            
            try:
                self.rate_limit_wait()
                
                response = self.session.get(self.base_url, params=params, timeout=30)
                
                if response.status_code != 200:
                    print(f"  âš ï¸  HTTP {response.status_code} ì—ëŸ¬")
                    if response.status_code == 429:
                        print("  Rate limit ì´ˆê³¼, 60ì´ˆ ëŒ€ê¸°...")
                        time.sleep(60)
                        continue
                    break
                
                data = response.json()
                posts = data.get('data', [])
                
                if not posts:
                    print(f"  ë” ì´ìƒ ë°ì´í„° ì—†ìŒ")
                    break
                
                for post in posts:
                    total_processed += 1
                    
                    title = post.get('title', '')
                    selftext = post.get('selftext', '')
                    combined_text = f"{title} {selftext}"
                    
                    found_tickers = self.extract_tickers(combined_text, target_tickers)
                    
                    if found_tickers:
                        for ticker in found_tickers:
                            created_utc = post.get('created_utc', 0)
                            
                            post_data = {
                                'source': 'reddit',
                                'subreddit': subreddit_name,
                                'subreddit_style': self.subreddits_config[subreddit_name]['style'],
                                'subreddit_strategy': self.subreddits_config[subreddit_name]['strategy'],
                                'ticker': ticker,
                                'title': title,
                                'selftext': selftext[:1000] if selftext else '',
                                'upvote_ratio': post.get('upvote_ratio', 0),
                                'score': post.get('score', 0),
                                'num_comments': post.get('num_comments', 0),
                                'created_utc': created_utc,
                                'created_date': datetime.fromtimestamp(created_utc).strftime('%Y-%m-%d %H:%M:%S'),
                                'year': year,
                                'quarter': quarter,
                                'author': post.get('author', '[deleted]'),
                                'author_flair_text': post.get('author_flair_text', None),
                                'url': post.get('url', ''),
                                'permalink': f"https://reddit.com{post.get('permalink', '')}"
                            }
                            results.append(post_data)
                            matched_count += 1
                    
                    # ë‹¤ìŒ í˜ì´ì§€ë¥¼ ìœ„í•œ timestamp ì—…ë°ì´íŠ¸
                    before_timestamp = post.get('created_utc', before_timestamp)
                
                print(f"  ğŸ“Š ì²˜ë¦¬: {total_processed}ê°œ | ë§¤ì¹­: {matched_count}/{target_count}ê°œ")
                
                # ëª©í‘œ ë‹¬ì„± ì‹œ ì¢…ë£Œ
                if matched_count >= target_count:
                    break
                
            except requests.exceptions.Timeout:
                print(f"  â±ï¸  íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„...")
                time.sleep(5)
                continue
                
            except Exception as e:
                print(f"  âŒ ì—ëŸ¬: {str(e)}")
                break
        
        print(f"âœ… ì™„ë£Œ: {len(results)}ê°œ ë°ì´í„° ìˆ˜ì§‘")
        return results
    
    def crawl_all_quarters(self, start_year: int, end_year: int,
                          target_tickers: Set[str], 
                          posts_per_quarter: int = 1000) -> pd.DataFrame:
        """
        ëª¨ë“  ì„œë¸Œë ˆë”§ì˜ ë¶„ê¸°ë³„ ë°ì´í„° í¬ë¡¤ë§
        
        Args:
            start_year: ì‹œì‘ ì—°ë„
            end_year: ì¢…ë£Œ ì—°ë„
            target_tickers: ì°¾ì„ í‹°ì»¤ ì„¸íŠ¸
            posts_per_quarter: ë¶„ê¸°ë‹¹ ëª©í‘œ ê²Œì‹œë¬¼ ìˆ˜
            
        Returns:
            ì „ì²´ ë°ì´í„° DataFrame
        """
        all_data = []
        current_year = datetime.now().year
        current_quarter = (datetime.now().month - 1) // 3 + 1
        
        total_tasks = 0
        for year in range(start_year, end_year + 1):
            for quarter in range(1, 5):
                if year == current_year and quarter > current_quarter:
                    continue
                total_tasks += len(self.subreddits_config)
        
        completed = 0
        
        for year in range(start_year, end_year + 1):
            for quarter in range(1, 5):
                # ë¯¸ë˜ ë¶„ê¸°ëŠ” ìŠ¤í‚µ
                if year == current_year and quarter > current_quarter:
                    continue
                
                for subreddit_name in self.subreddits_config.keys():
                    completed += 1
                    print(f"\n\nğŸ“ ì§„í–‰ë¥ : {completed}/{total_tasks}")
                    
                    try:
                        quarter_data = self.crawl_quarter(
                            subreddit_name, year, quarter,
                            target_tickers, posts_per_quarter
                        )
                        all_data.extend(quarter_data)
                        
                    except KeyboardInterrupt:
                        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
                        print(f"í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(all_data)}ê°œ")
                        if all_data:
                            return pd.DataFrame(all_data)
                        raise
                        
                    except Exception as e:
                        print(f"âŒ ì—ëŸ¬: r/{subreddit_name} {year}Q{quarter} - {str(e)}")
                        continue
        
        return pd.DataFrame(all_data)
    
    def save_data(self, df: pd.DataFrame, base_filename: str = 'reddit_ticker_data'):
        """
        ë°ì´í„°ë¥¼ CSVì™€ JSONìœ¼ë¡œ ì €ì¥
        
        Args:
            df: ì €ì¥í•  DataFrame
            base_filename: ê¸°ë³¸ íŒŒì¼ëª…
        """
        if df.empty:
            print("âš ï¸  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # CSV ì €ì¥
        csv_file = f"{base_filename}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ CSV ì €ì¥: {csv_file}")
        
        # JSON ì €ì¥
        json_file = f"{base_filename}.json"
        df.to_json(json_file, orient='records', force_ascii=False, indent=2)
        print(f"ğŸ’¾ JSON ì €ì¥: {json_file}")
        
        # í†µê³„ ì¶œë ¥
        print(f"\n{'='*60}")
        print(f"ğŸ“Š í¬ë¡¤ë§ í†µê³„")
        print(f"{'='*60}")
        print(f"ì´ ë°ì´í„° ìˆ˜: {len(df):,}ê°œ\n")
        
        print("ğŸ“Œ ì„œë¸Œë ˆë”§ë³„ ë¶„í¬:")
        print(df['subreddit'].value_counts().to_string())
        
        print(f"\nğŸ“ˆ í‹°ì»¤ë³„ ë¶„í¬:")
        print(df['ticker'].value_counts().head(20).to_string())
        
        print(f"\nğŸ“… ì—°ë„/ë¶„ê¸°ë³„ ë¶„í¬:")
        quarter_dist = df.groupby(['year', 'quarter']).size().sort_index()
        print(quarter_dist.to_string())
        
        print(f"\nğŸ’ª í‰ê·  Score (í™”ë ¥):")
        avg_score = df.groupby('subreddit')['score'].mean().round(1)
        print(avg_score.to_string())
        
        print(f"\nğŸ’¬ í‰ê·  ëŒ“ê¸€ ìˆ˜:")
        avg_comments = df.groupby('subreddit')['num_comments'].mean().round(1)
        print(avg_comments.to_string())


# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================
if __name__ == "__main__":
    print("="*60)
    print("Reddit ì£¼ì‹ í‹°ì»¤ í¬ë¡¤ëŸ¬ v2.0 (PullPush.io)")
    print("="*60)
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = RedditTickerCrawler()
    
    # íƒ€ê²Ÿ í‹°ì»¤ ì„¤ì •
    target_tickers = {
        # ì£¼ìš” í…Œí¬ ì£¼ì‹
        'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'META', 'TSLA', 'NVDA', 'AMD',
        
        # ìœ ëª… ë°ˆì£¼
        'GME', 'AMC', 'BB', 'BBBY', 'NOK',
        
        # ì—ë„ˆì§€/ì„ìœ 
        'OXY', 'XOM', 'CVX', 'COP',
        
        # ê¸°íƒ€ ì¸ê¸° ì¢…ëª©
        'PLTR', 'BABA', 'NIO', 'SOFI', 'COIN', 'HOOD',
        
        # ETF
        'SPY', 'QQQ', 'IWM', 'DIA', 'VOO'
    }
    
    print(f"\nğŸ¯ íƒ€ê²Ÿ í‹°ì»¤ ({len(target_tickers)}ê°œ):")
    print(f"{', '.join(sorted(target_tickers))}\n")
    
    # í¬ë¡¤ë§ íŒŒë¼ë¯¸í„°
    START_YEAR = 2023
    END_YEAR = 2024
    POSTS_PER_QUARTER = 1000  # ë¶„ê¸°ë‹¹ ëª©í‘œ ê°œìˆ˜ (í‹°ì»¤ ë§¤ì¹­ëœ ê²ƒ)
    
    print(f"ğŸ“… ê¸°ê°„: {START_YEAR}ë…„ ~ {END_YEAR}ë…„ (ë¶„ê¸°ë³„)")
    print(f"ğŸ“Š ëª©í‘œ: ë¶„ê¸°ë‹¹ {POSTS_PER_QUARTER}ê°œ (ì„œë¸Œë ˆë”§ë‹¹)")
    print(f"â±ï¸  ì˜ˆìƒ ì†Œìš”ì‹œê°„: {(END_YEAR-START_YEAR+1)*4*3*5} ~ 10ë¶„")
    print(f"\nâš ï¸  Rate Limit: ì‹œê°„ë‹¹ 1000 ìš”ì²­, ë¶„ë‹¹ 15 ìš”ì²­")
    print(f"ğŸ’¡ ì¤‘ë‹¨í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš” (í˜„ì¬ê¹Œì§€ ë°ì´í„°ëŠ” ì €ì¥ë¨)\n")
    
    input("ğŸš€ Enterë¥¼ ëˆŒëŸ¬ í¬ë¡¤ë§ ì‹œì‘...")
    
    try:
        # í¬ë¡¤ë§ ì‹œì‘
        start_time = time.time()
        
        df = crawler.crawl_all_quarters(
            start_year=START_YEAR,
            end_year=END_YEAR,
            target_tickers=target_tickers,
            posts_per_quarter=POSTS_PER_QUARTER
        )
        
        elapsed = time.time() - start_time
        print(f"\n\nâ±ï¸  ì´ ì†Œìš”ì‹œê°„: {elapsed/60:.1f}ë¶„")
        
        # ë°ì´í„° ì €ì¥
        if not df.empty:
            crawler.save_data(df, f'reddit_ticker_data_{START_YEAR}_{END_YEAR}')
            print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        else:
            print(f"\nâš ï¸  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        if not df.empty:
            crawler.save_data(df, f'reddit_ticker_data_partial_{START_YEAR}_{END_YEAR}')
            print(f"âœ… ë¶€ë¶„ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
    
    except Exception as e:
        print(f"\nâŒ ì¹˜ëª…ì  ì—ëŸ¬: {str(e)}")
        import traceback
        traceback.print_exc()