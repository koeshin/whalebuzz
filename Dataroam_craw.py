import pandas as pd
from playwright.sync_api import sync_playwright
from io import StringIO
import time
import random
import re

# 1. ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ (ê²€ì¦ëœ ì½”ë“œ ì‚¬ìš©)
TARGET_GURUS = [
    {"code": "SAM",     "name": "Scion Asset Mgmt (Michael Burry)"},
    {"code": "BAUPOST", "name": "Baupost Group (Seth Klarman)"}, 
    {"code": "BRK",     "name": "Berkshire Hathaway (Warren Buffett)"},
    # í•„ìš”í•œ ë§Œí¼ ì¶”ê°€
]

def scrape_ticker_history():
    all_history_data = []

    with sync_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹¤í–‰
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        page = context.new_page()

        print(f"ğŸ”¥ [ì¢…ëª© ì¤‘ì‹¬] íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n")

        for guru in TARGET_GURUS:
            guru_code = guru["code"]
            guru_name = guru["name"]
            
            print(f"--- [{guru_name}] ì¢…ëª© ë°œêµ´ ì‹œì‘ ---")

            # Step 1: Activity í˜ì´ì§€ì—ì„œ 'ê±´ë“œë¦° ì¢…ëª©' ë¦¬ìŠ¤íŠ¸ í™•ë³´
            # typ=a (All)ë¡œ í•´ì•¼ ë§¤ìˆ˜/ë§¤ë„í•œ ëª¨ë“  ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            url_activity = f"https://www.dataroma.com/m/m_activity.php?m={guru_code}&typ=a"
            
            unique_tickers = set() # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ Set
            
            try:
                page.goto(url_activity, timeout=30000)
                page.wait_for_selector("#grid", timeout=10000)
                
                # í…Œì´ë¸” ë‚´ì˜ ëª¨ë“  ë§í¬(a íƒœê·¸) ì¤‘ì—ì„œ 'hist.php'ë¡œ ê°€ëŠ” ê²ƒë§Œ ì°¾ìŒ
                # href ì˜ˆì‹œ: hist.php?f=SAM&s=LULU
                links = page.locator("#grid td.stock a").all()
                
                for link in links:
                    href = link.get_attribute("href")
                    # ì •ê·œì‹ìœ¼ë¡œ í‹°ì»¤(s=???) ì¶”ì¶œ
                    match = re.search(r's=([^&]+)', href)
                    if match:
                        ticker = match.group(1)
                        unique_tickers.add(ticker)
                
                print(f"   ğŸ‘‰ ì´ {len(unique_tickers)}ê°œì˜ ê³ ìœ  ì¢…ëª© ë°œê²¬: {list(unique_tickers)[:5]} ...")

            except Exception as e:
                print(f"   âŒ Activity í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {e}")
                continue

            # Step 2: ê° í‹°ì»¤ë³„ ìƒì„¸ íˆìŠ¤í† ë¦¬ í˜ì´ì§€ ìˆœíšŒ
            # (í‹°ì»¤ê°€ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ, ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì¤Œ)
            count = 0
            for ticker in unique_tickers:
                count += 1
                history_url = f"https://www.dataroma.com/m/hist/hist.php?f={guru_code}&s={ticker}"
                print(f"   [{count}/{len(unique_tickers)}] {ticker} ë¶„ì„ ì¤‘...", end="\r") # í•œ ì¤„ë¡œ ì¶œë ¥

                try:
                    page.goto(history_url, timeout=20000)
                    # íˆìŠ¤í† ë¦¬ í…Œì´ë¸” ëŒ€ê¸°
                    try:
                        page.wait_for_selector("#grid", timeout=5000)
                    except:
                        # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ë„ ìˆìŒ
                        continue

                    html = page.content()
                    dfs = pd.read_html(StringIO(html))
                    
                    if dfs:
                        hist_df = dfs[0]
                        
                        # ì»¬ëŸ¼ ì •ë¦¬ (Period, Shares, % of Portfolio, Activity, % Change, Price, Value ë“±)
                        # ì‚¬ì´íŠ¸ êµ¬ì¡°ìƒ ì»¬ëŸ¼ëª…ì´ ì¡°ê¸ˆì”© ë‹¤ë¥¼ ìˆ˜ ìˆì–´ í•µì‹¬ë§Œ ë‚¨ê¹€
                        
                        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                        hist_df.insert(0, "Manager", guru_name)
                        hist_df.insert(1, "Ticker", ticker)
                        
                        all_history_data.append(hist_df)
                        
                except Exception as e:
                    # íŠ¹ì • ì¢…ëª© ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    pass
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´ (í•„ìˆ˜)
                time.sleep(random.uniform(1.0, 2.0))
            
            print(f"\n   âœ… {guru_name} ì™„ë£Œ.\n")

        browser.close()

    # ê²°ê³¼ ì €ì¥
    if all_history_data:
        print("\nğŸ“Š ë°ì´í„° ë³‘í•© ì¤‘...")
        master_df = pd.concat(all_history_data, ignore_index=True)
        
        # ë³´ê¸° ì¢‹ê²Œ ì»¬ëŸ¼ ì •ë¦¬ (ì˜µì…˜)
        # ë³´í†µ ì»¬ëŸ¼: Period, Shares, % of Portfolio, Activity, % Change to Portfolio, Reported Price
        
        filename = "Guru_Ticker_Full_History.csv"
        master_df.to_csv(filename, index=False, encoding="utf-8-sig")
        print(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! '{filename}' ì €ì¥ë¨. (ì´ {len(master_df)}ê±´ì˜ ê±°ë˜ ê¸°ë¡)")
        
        # ìƒ˜í”Œ ì¶œë ¥
        print(master_df.head())
        return master_df
    else:
        print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

if __name__ == "__main__":
    scrape_ticker_history()