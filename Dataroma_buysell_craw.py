import pandas as pd
from playwright.sync_api import sync_playwright
from io import StringIO
import time
import random
import re
import os  # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ìš©

# 1. ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸
# 1. ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ (ì´ 21ê°œ, Dataroma ê²€ì¦ ì™„ë£Œ)
TARGET_GURUS = [
    # --- Value (ê°€ì¹˜íˆ¬ì) ---
    {"code": "BRK",     "name": "Berkshire Hathaway (Buffett)", "style": "Value"},
    {"code": "BAUPOST", "name": "Baupost Group (Klarman)",     "style": "Value"},
    {"code": "SAM",     "name": "Scion Asset Mgmt (Burry)",    "style": "Value"},
    {"code": "HC",      "name": "Himalaya Capital (Li Lu)",    "style": "Value"},
    {"code": "PI",     "name": "Pabrai Investments(Pabrai)",       "style": "Value"}, ## ë‹¤ì‹œ ëŒë¦¬ê¸°
    {"code": "FS",      "name": "Fundsmith (Terry Smith)",     "style": "Value"},
    {"code": "oaklx",     "name": "Oakmark (Bill Nygren)",       "style": "Value"}, ## ë‹¤ì‹œ ëŒë¦¬ê¸°

    # --- Growth (ì„±ì¥ì£¼/Tiger Cubs) ---
    {"code": "TGM",     "name": "Tiger Global (Chase Coleman)","style": "Growth"},
    {"code": "AM",      "name": "Appaloosa (David Tepper)",    "style": "Growth"},
    {"code": "vg",     "name": "Viking Global (Halvorsen)",   "style": "Growth"}, ## ë‹¤ì‹œ ëŒë¦¬ê¸°
    {"code": "LPC",     "name": "Lone Pine (Stephen Mandel)",  "style": "Growth"},
    {"code": "MC",      "name": "Maverick Capital (Lee Ainslie)","style": "Growth"},
    {"code": "AC",    "name": "Akre Capital (Chuck Akre)",   "style": "Growth"}, # ë‹¤ì‹œ ëŒë¦¬ê¸°
    {"code": "tci",     "name": "TCI Fund (Chris Hohn)",       "style": "Growth"},

    # --- Activist / Deep Value (í–‰ë™ì£¼ì˜) ---
    {"code": "PSC",     "name": "Pershing Square (Ackman)",    "style": "Activist"},
    {"code": "IC",      "name": "Icahn Capital (Carl Icahn)",  "style": "Activist"},
    {"code": "TP",      "name": "Third Point (Dan Loeb)",      "style": "Activist"},
    {"code": "GL",      "name": "Greenlight (David Einhorn)",  "style": "Activist"},
    {"code": "TRI",     "name": "Trian Partners (Nelson Peltz)","style": "Activist"},
    {"code": "STAR",    "name": "Starboard Value (Jeff Smith)","style": "Activist"},
    {"code": "FAIRX",   "name": "Fairholme (Bruce Berkowitz)", "style": "Activist"},
]
FILENAME = "Guru_History_21_Legends.csv"

def scrape_and_save_incremental():
    # ì‹œì‘ ì „ì— ê¸°ì¡´ íŒŒì¼ì´ ìˆë‹¤ë©´ ì•ˆë‚´ ë©”ì‹œì§€ (í˜¹ì€ ì‚­ì œ)
    if os.path.exists(FILENAME):
        print(f"â„¹ï¸ ì•Œë¦¼: '{FILENAME}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë’¤ì— ì´ì–´ì„œ ì €ì¥í•©ë‹ˆë‹¤.")
    else:
        print(f"â„¹ï¸ ì•Œë¦¼: ìƒˆë¡œìš´ íŒŒì¼ '{FILENAME}'ì„ ìƒì„±í•©ë‹ˆë‹¤.")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        # Contextë¥¼ í•œ ë²ˆ ë§Œë“¤ê³  ê³„ì† ì¬ì‚¬ìš©í•˜ë˜, í˜ì´ì§€ëŠ” ë‹«ì•„ì¤ë‹ˆë‹¤.
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )

        print(f"\nğŸ”¥ [ì•ˆì „ ëª¨ë“œ] í•œ ëª…ì”© ìˆ˜ì§‘í•˜ê³  ì¦‰ì‹œ ì €ì¥í•©ë‹ˆë‹¤.\n")

        for i, guru in enumerate(TARGET_GURUS):
            guru_code = guru["code"]
            guru_name = guru["name"]
            guru_style = guru["style"]
            
            # [ì¤‘ìš”] í•œ ëª…ë¶„ ë°ì´í„°ë¥¼ ë‹´ì„ ì„ì‹œ ë¦¬ìŠ¤íŠ¸ (ë§¤ë²ˆ ì´ˆê¸°í™”ë¨)
            current_guru_data = []
            
            print(f"--- [{i+1}/{len(TARGET_GURUS)}] {guru_name} ({guru_style}) ì‹œì‘ ---")

            page = context.new_page()
            
            try:
                # 1. Activity í˜ì´ì§€ ì ‘ì† & í‹°ì»¤ ìˆ˜ì§‘
                url_activity = f"https://www.dataroma.com/m/m_activity.php?m={guru_code}&typ=a"
                unique_tickers = set()
                
                try:
                    page.goto(url_activity, timeout=30000)
                    try:
                        page.wait_for_selector("#grid", timeout=5000)
                    except:
                        print("   âš ï¸ í…Œì´ë¸” ë¡œë”© ì‹¤íŒ¨ (ë°ì´í„° ì—†ìŒ)")

                    # stock.php ë§í¬ ì°¾ê¸°
                    target_links = page.locator('a[href*="stock.php?sym="]').all()
                    for link in target_links:
                        href = link.get_attribute("href")
                        if href:
                            match = re.search(r'sym=([^&]+)', href)
                            if match:
                                unique_tickers.add(match.group(1))
                    
                    print(f"   ğŸ‘‰ {len(unique_tickers)}ê°œ ì¢…ëª© ë°œê²¬")

                except Exception as e:
                    print(f"   âŒ Activity ì—ëŸ¬: {e}")

                # 2. ìƒì„¸ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘
                count = 0
                for ticker in unique_tickers:
                    count += 1
                    history_url = f"https://www.dataroma.com/m/hist/hist.php?f={guru_code}&s={ticker}"
                    
                    print(f"   [{count}/{len(unique_tickers)}] {ticker}...", end="\r")

                    try:
                        page.goto(history_url, timeout=20000)
                        try:
                            page.wait_for_selector("#grid", timeout=2000)
                        except:
                            continue 

                        html = page.content()
                        dfs = pd.read_html(StringIO(html))
                        
                        if dfs:
                            hist_df = max(dfs, key=len)
                            if len(hist_df) > 1:
                                # ë©”íƒ€ë°ì´í„° ì‚½ì…
                                hist_df.insert(0, "Manager", guru_name)
                                hist_df.insert(1, "Style", guru_style) 
                                hist_df.insert(2, "Ticker", ticker)
                                
                                # ì„ì‹œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                                current_guru_data.append(hist_df)
                            
                    except Exception:
                        pass
                    
                    # ë”œë ˆì´ (ë„ˆë¬´ ë¹ ë¥´ë©´ ì°¨ë‹¨ë˜ë¯€ë¡œ ì ì ˆíˆ ìœ ì§€)
                    time.sleep(random.uniform(0.5, 0.8))

            except Exception as e:
                print(f"   âŒ ì¹˜ëª…ì  ì—ëŸ¬: {e}")
            
            finally:
                page.close() # í˜ì´ì§€ ë‹«ì•„ì„œ ë©”ëª¨ë¦¬ í™•ë³´

            # 3. [ì €ì¥ ë‹¨ê³„] í•œ ëª… ëë‚  ë•Œë§ˆë‹¤ íŒŒì¼ì— ì“°ê¸°
            if current_guru_data:
                print(f"\n   ğŸ’¾ {guru_name} ë°ì´í„° ì €ì¥ ì¤‘... ", end="")
                
                # DataFrame ë³€í™˜
                df_to_save = pd.concat(current_guru_data, ignore_index=True)
                
                # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” í¬í•¨(True), ìˆìœ¼ë©´ í—¤ë” ëºŒ(False)
                # mode='a'ëŠ” append(ì´ì–´ì“°ê¸°) ëª¨ë“œì…ë‹ˆë‹¤.
                file_exists = os.path.exists(FILENAME)
                
                df_to_save.to_csv(
                    FILENAME, 
                    mode='a', 
                    header=not file_exists, # íŒŒì¼ì´ ì—†ì„ ë•Œë§Œ í—¤ë” ì‘ì„±
                    index=False, 
                    encoding="utf-8-sig"
                )
                
                print(f"ì™„ë£Œ! (+{len(df_to_save)}í–‰)")
                
                # ë©”ëª¨ë¦¬ì—ì„œ ì‚­ì œ (Explicit Garbage Collection)
                del df_to_save
                del current_guru_data
            else:
                print(f"\n   âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            print("------------------------------------------------")

        browser.close()
        print(f"\nğŸ‰ ëª¨ë“  ì‘ì—… ì¢…ë£Œ. ê²°ê³¼ íŒŒì¼: {FILENAME}")

if __name__ == "__main__":
    scrape_and_save_incremental()